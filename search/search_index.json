{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"oterm","text":"<p>the terminal client for Ollama.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>intuitive and simple terminal UI, no need to run servers, frontends, just type <code>oterm</code> in your terminal.</li> <li>multiple persistent chat sessions, stored together with system prompt &amp; parameter customizations in sqlite.</li> <li>support for Model Context Protocol (MCP) tools &amp; prompts integration.</li> <li>can use any of the models you have pulled in Ollama, or your own custom models.</li> <li>allows for easy customization of the model's system prompt and parameters.</li> <li>supports tools integration for providing external information to the model.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>See the Installation section.</p>"},{"location":"#using-oterm","title":"Using oterm","text":"<p>In order to use <code>oterm</code> you will need to have the Ollama server running. By default it expects to find the Ollama API running on <code>http://127.0.0.1:11434</code>. If you are running Ollama inside docker or on a different host/port, use the <code>OLLAMA_HOST</code> environment variable to customize the host/port. Alternatively you can use <code>OLLAMA_URL</code> to specify the full http(s) url. Setting <code>OTERM_VERIFY_SSL</code> to <code>False</code> will disable SSL verification.</p> <pre><code>OLLAMA_URL=http://host:port\n</code></pre> <p>To start <code>oterm</code> simply run:</p> <pre><code>oterm\n</code></pre> <p>If you installed oterm using <code>uvx</code>, you can also start it using:</p> <pre><code>uvx oterm\n</code></pre>"},{"location":"#screenshots","title":"Screenshots","text":"<p> The splash screen animation that greets users when they start oterm.</p> <p> A view of the chat interface, showcasing the conversation between the user and the model.</p> <p> oterm supports multiple themes, allowing users to customize the appearance of the interface.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"app_config/","title":"Configuration","text":""},{"location":"app_config/#app-configuration","title":"App configuration","text":"<p>The app configuration is stored as JSON in <code>config.json</code> in a directory specific to your operating system. By default:</p> <ul> <li>Linux: <code>~/.local/share/oterm</code></li> <li>macOS: <code>~/Library/Application Support/oterm</code></li> <li>Windows: <code>C:/Users/&lt;USER&gt;/AppData/Roaming/oterm</code></li> </ul> <p>On Linux &amp; MacOS we honour the <code>XDG_DATA_HOME</code> environment variable. In that case, he directory will be <code>${XDG_DATA_HOME}/oterm</code>.</p> <p>If in doubt you can get the directory where <code>config.json</code> can be found by running <code>oterm --data-dir</code> or <code>uvx oterm --data-dir</code> if you installed oterm using uvx.</p> <p>You can set the following options in the configuration file: <pre><code>{ \"splash-screen\": true }\n</code></pre></p> <p><code>splash-screen</code> controls whether the splash screen is shown on startup.</p>"},{"location":"app_config/#key-bindings","title":"Key bindings","text":"<p>We strive to have sane default key bindings, but there will always be cases where your terminal emulator or shell will interfere. You can customize select keybindings by editing the app config <code>config.json</code> file. The following are the defaults:</p> <pre><code>{\n  ...\n  \"keymap\": {\n    \"next.chat\": \"ctrl+tab\",\n    \"prev.chat\": \"ctrl+shift+tab\",\n    \"quit\": \"ctrl+q\",\n    \"newline\": \"shift+enter\"\n  }\n}\n</code></pre>"},{"location":"app_config/#chat-storage","title":"Chat storage","text":"<p>All your chat sessions are stored locally in a sqlite database. You can customize the directory where the database is stored by setting the <code>OTERM_DATA_DIR</code> environment variable.</p> <p>You can find the location of the database by running <code>oterm --db</code>.</p>"},{"location":"commands/","title":"Commands & shortcuts","text":""},{"location":"commands/#commands","title":"Commands","text":"<p>By pressing ^ Ctrl+p you can access the command palette from where you can perform most of the chat actions. The following commands are available:</p> <ul> <li><code>New chat</code> - create a new chat session</li> <li><code>Edit chat parameters</code> - edit the current chat session (change system prompt, parameters or format)</li> <li><code>Rename chat</code> - rename the current chat session</li> <li><code>Export chat</code> - export the current chat session as markdown</li> <li><code>Delete chat</code> - delete the current chat session</li> <li><code>Clear chat</code> - clear the chat history, preserving model and system prompt customizations</li> <li><code>Regenerate last Ollama message</code> - regenerates the last message from Ollama (will override the <code>seed</code> for the specific message with a random one.) Useful if you want to change the system prompt or parameters or just want to try again.</li> <li><code>Pull model</code> - pull a model or update an existing one.</li> <li><code>Change theme</code> - choose among the available themes.</li> <li><code>Show logs</code> - shows the logs of the current oterm session.</li> </ul>"},{"location":"commands/#keyboard-shortcuts","title":"Keyboard shortcuts","text":"<p>The following keyboard shortcuts are supported:</p> <ul> <li> <p>^ Ctrl+q - quit</p> </li> <li> <p>^ Ctrl+m - switch to multiline input mode</p> </li> <li>^ Ctrl+i - select an image to include with the next message</li> <li>\u2191/\u2193 (while messages are focused) - navigate through the messages</li> <li>\u2191 (while prompt is focused)    - navigate through history of previous prompts</li> <li> <p>^ Ctrl+l - show logs</p> </li> <li> <p>^ Ctrl+n - open a new chat</p> </li> <li> <p>^ Ctrl+Backspace - close the current chat</p> </li> <li> <p>^ Ctrl+Tab - open the next chat</p> </li> <li>^ Ctrl+Shift+Tab - open the previous chat</li> </ul> <p>In multiline mode, you can press Enter to send the message, or Shift+Enter to add a new line at the cursor.</p> <p>While Ollama is inferring the next message, you can press Esc to cancel the inference.</p> <p>Note</p> <p>Some of the shortcuts may not work in a certain context, if they are overridden by the widget in focus. For example pressing \u2191 while the prompt is in multi-line mode.</p> <p>If the key bindings clash with your terminal, it is possible to change them by editing the configuration file. See Configuration.</p>"},{"location":"commands/#copy-paste","title":"Copy / Paste","text":"<p>It is difficult to properly support copy/paste in terminal applications. You can copy blocks to your clipboard as such:</p> <ul> <li>clicking a message will copy it to the clipboard.</li> <li>clicking a code block will only copy the code block to the clipboard.</li> </ul> <p>For most terminals there exists a key modifier you can use to click and drag to manually select text. For example: * <code>iTerm</code> Option key. * <code>Gnome Terminal</code> Shift key. * <code>Windows Terminal</code> Shift key.</p> <p> The image selection interface.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>Note</p> <p>Ollama needs to be installed and running in order to use <code>oterm</code>. Please follow the Ollama Installation Guide.</p> <p>Using <code>uvx</code>:</p> <pre><code>uvx oterm\n</code></pre> <p>Using <code>brew</code> for MacOS:</p> <pre><code>brew install oterm\n</code></pre> <p>Note</p> <p>Since version <code>0.13.1</code>, <code>oterm</code> is in the official <code>homebrew/core</code> repository. If you have installed <code>oterm</code> by tapping  <code>ggozad/formulas</code> you can now remove the tap and reinstall <code>oterm</code>.</p> <p>Using <code>yay</code> (or any AUR helper) for Arch Linux, thanks goes to Daniel Chesters for maintaining the package:</p> <pre><code>yay -S oterm\n</code></pre> <p>Using <code>nix-env</code> on NixOs, thanks goes to Ga\u00ebl James for maintaining the package:</p> <pre><code>nix-env -iA nixpkgs.oterm\n</code></pre> <p>Using <code>pip</code>:</p> <pre><code>pip install oterm\n</code></pre> <p>Using <code>pkg</code> for FreeBSD, thanks goes to Nicola Vitale for maintaining the package:</p> <pre><code>pkg install misc/py-oterm\n</code></pre> <p>Using <code>x-cmd</code>:</p> <pre><code>x install oterm\n</code></pre>"},{"location":"installation/#updating-oterm","title":"Updating oterm","text":"<p>To update oterm to the latest version, you can use the same method you used for installation:</p> <p>Using <code>uvx</code>:</p> <pre><code>uvx oterm@latest\n</code></pre> <p>Using <code>brew</code> for MacOS:</p> <p><pre><code>brew upgrade oterm\n</code></pre> Using 'yay' (or any AUR helper) for Arch Linux:</p> <p><pre><code>yay -Syu oterm\n</code></pre> Using <code>pip</code>:</p> <pre><code>pip install --upgrade oterm\n</code></pre> <p>Using <code>pkg</code> for FreeBSD:</p> <pre><code>pkg upgrade misc/py-oterm\n</code></pre>"},{"location":"parameters/","title":"Chat parameters","text":"<p>When creating a new chat, you may select the model you want and customize the following:</p> <ul> <li><code>system</code> instruction prompt</li> <li><code>tools</code> used. See Tools for more information on how to make tools available.</li> <li>chat <code>parameters</code> (such as context length, seed, temperature etc) passed to the model. For a list of all supported parameters refer to the Ollama documentation.</li> <li>Ouput <code>format</code>/structured output. In the format field you can use Ollama's Structured Output specifying the full format as a JSON schema. Leaving the field empty (default) will return the output as text.</li> <li>enable <code>thinking</code> for models that support it.</li> </ul> <p>Note</p> <p>When <code>thinking</code> is enabled you will observe the model thinking while generating its response. The thinking process is not persisted in the database in order to save context, so you will not see it on later sessions.</p> <p>You can also \"edit\" an existing chat to change the system prompt, parameters, tools or format. Note, that the model cannot be changed once the chat has started.</p> <p> The model selection screen, allowing users to choose and customize available models.</p>"},{"location":"rag_example/","title":"RAG with haiku.rag","text":"<p>Transform oterm into a powerful RAG (Retrieval-Augmented Generation) system by integrating with haiku.rag (from the <code>oterm</code>'s author'), a SQLite-based RAG library that works seamlessly with oterm through MCP.</p>"},{"location":"rag_example/#what-is-haikurag","title":"What is haiku.rag?","text":"<p>haiku.rag is a comprehensive RAG library that:</p> <ul> <li>Uses only SQLite (no external vector databases needed)</li> <li>Supports 40+ file formats (PDF, DOCX, HTML, Markdown, code files, URLs)</li> <li>Provides hybrid search (semantic + full-text) with Reciprocal Rank Fusion</li> <li>Works with multiple embedding providers (Ollama, OpenAI, VoyageAI)</li> <li>Offers built-in reranking and question-answering capabilities</li> <li>Exposes functionality through MCP tools for seamless AI assistant integration</li> </ul>"},{"location":"rag_example/#configuration","title":"Configuration","text":"<p>Add the haiku-rag MCP server to your oterm configuration. Edit your <code>config.json</code> file (run <code>oterm --data-dir</code> to find its location) and add:</p> <pre><code>{\n  \"mcpServers\": {\n    \"haiku-rag\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"haiku-rag\",\n        \"serve\",\n        \"--stdio\",\n        \"--db\",\n        \"/path/to/your/rag.db\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Replace <code>/path/to/your/rag.db</code> with the path where you want to store your RAG database.</p>"},{"location":"rag_example/#available-rag-tools","title":"Available RAG Tools","text":"<p>Once configured, oterm will have access to powerful RAG capabilities through these MCP tools:</p> <ul> <li>Add documents: Upload text, files, or URLs to your knowledge base</li> <li>List documents: View all documents in your RAG database</li> <li>Delete documents: Remove documents from your knowledge base</li> <li>Update documents: Modify existing documents</li> <li>Search documents: Performs hybrid search on your knowledge base</li> </ul>"},{"location":"rag_example/#usage-examples","title":"Usage Examples","text":""},{"location":"rag_example/#1-building-a-personal-knowledge-base","title":"1. Building a Personal Knowledge Base","text":"<p>Start a new chat in oterm and use the RAG tools to build your knowledge base:</p> <p><code>Add this document to my knowledge base: \"Machine Learning is a subset of artificial intelligence that focuses on algorithms that can learn from data...\"</code></p>"},{"location":"rag_example/#2-adding-files","title":"2. Adding Files","text":"<p>You can add various file types:</p> <p><code>Please add the PDF file at /Users/me/Documents/research_paper.pdf to my RAG database</code></p>"},{"location":"rag_example/#3-adding-web-content","title":"3. Adding Web Content","text":"<p>Add content from URLs:</p> <p><code>Add the content from https://example.com/article to my knowledge base</code></p>"},{"location":"rag_example/#4-searching-your-knowledge-base","title":"4. Searching Your Knowledge Base","text":"<p>Perform semantic searches:</p> <p><code>Search my knowledge base for information about \"neural networks\"</code></p>"},{"location":"rag_example/#5-question-answering","title":"5. Question Answering","text":"<p>Ask questions about your documents:</p> <p><code>Based on my knowledge base, what are the main differences between supervised and unsupervised learning?</code></p>"},{"location":"rag_example/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"rag_example/#custom-embedding-providers","title":"Custom Embedding Providers","text":"<p>Configure haiku.rag to use different embedding providers, rerankers, models by setting environment variables. See <code>haiku.rag</code> documentation</p> <pre><code>{\n  \"mcpServers\": {\n    \"haiku-rag\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"haiku-rag\",\n        \"serve\",\n        \"--stdio\",\n        \"--db\",\n        \"/path/to/rag.db\"\n      ],\n      \"env\": {\n        \"EMBEDDINGS_PROVIDER\": \"ollama\",\n        \"EMBEDDINGS_MODEL\": \"nomic-embed-text\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"rag_example/#further-reading","title":"Further Reading","text":"<ul> <li>haiku.rag Documentation</li> <li>haiku.rag GitHub Repository</li> </ul>"},{"location":"rag_example/#example-use-cases","title":"Example Use Cases","text":""},{"location":"rag_example/#research-assistant","title":"Research Assistant","text":"<p>Build a personal research database by adding academic papers, articles, and notes. Ask questions across your entire research collection.</p>"},{"location":"rag_example/#documentation-helper","title":"Documentation Helper","text":"<p>Index your project's documentation, code comments, and README files. Get instant answers about your codebase.</p>"},{"location":"rag_example/#learning-companion","title":"Learning Companion","text":"<p>Add course materials, textbooks, and learning resources. Create a personalized tutor that can answer questions about your study materials.</p>"},{"location":"development/","title":"Development &amp; Debugging","text":""},{"location":"development/#inspecting-logs","title":"Inspecting logs","text":"<p>You can inspect basic logs from oterm by invoking the log viewer with ^ Ctrl+l or by using the command palette. This is particurly useful if you want to debug tool calling.</p> <p> oterm's internal log viewer showing the Brave Search MCP tool in action.</p>"},{"location":"development/#setup-for-development","title":"Setup for development","text":"<ul> <li>Create a virtual environment <pre><code>uv venv\n</code></pre></li> <li> <p>Activate the virtual environment <pre><code>source .venv/bin/activate\n# or on Windows\nsource .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install oterm <pre><code>uv pip install oterm\n</code></pre> or checkout the repository and install the oterm package from source <pre><code>git clone git@github.com:ggozad/oterm.git\nuv sync\n</code></pre></p> </li> </ul>"},{"location":"development/#debugging","title":"Debugging","text":"<ul> <li> <p>In order to inspect logs from oterm, open a new terminal and run: <pre><code>source .venv/bin/activate\ntextual console -x SYSTEM -x EVENT -x WORKER -x DEBUG\n</code></pre> This will start the textual console and listen all log messages from oterm, hiding some of the textual UI messsages.</p> </li> <li> <p>You can now start oterm in debug mode: <pre><code>source .venv/bin/activate\ntextual run -c --dev oterm\n</code></pre></p> </li> </ul>"},{"location":"development/#documentation","title":"Documentation","text":"<p>oterm uses mkdocs with material to generate the documentation. To build the documentation, run: <pre><code>source .venv/bin/activate\nmkdocs serve -o\n</code></pre> This will start a local server and open the documentation pages in your default web browser.</p>"},{"location":"mcp/","title":"Model Context Protocol","text":"<p><code>oterm</code> has support for Anthropic's open-source Model Context Protocol. While Ollama does not yet directly support the protocol, <code>oterm</code> attempts to bridge MCP servers with Ollama.</p> <p>To add an MCP server to <code>oterm</code>, simply add the server shim to oterm's config.json. The following MCP transports are supported</p>"},{"location":"mcp/#supported-mcp-features","title":"Supported MCP Features","text":""},{"location":"mcp/#tools","title":"Tools","text":"<p>By transforming MCP tools into Ollama tools <code>oterm</code> provides full support.</p> <p>Note</p> <p>Not all models are equipped to support tools. For those models that do not, the tool selection will be disabled.</p> <p>A lot of the smaller LLMs are not as capable with tools as larger ones you might be used to. If you experience issues with tools, try reducing the number of tools you attach to a chat, increase the context size, or use a larger LLM.</p> <p> oterm using the <code>git</code> MCP server to access its own repo.</p>"},{"location":"mcp/#prompts","title":"Prompts","text":"<p><code>oterm</code> supports MCP prompts. Use the \"Use MCP prompt\" command to invoke a form with the prompt. Submitting will insert the prompt messages into the chat.</p> <p> oterm displaying a test MCP prompt.</p>"},{"location":"mcp/#sampling","title":"Sampling","text":"<p><code>oterm</code> supports MCP sampling, acting as a geteway between Ollama and the servers it connects to. This way, an MCP server can request <code>oterm</code> to run a completion and even declare its model preferences and parameters!</p>"},{"location":"mcp/#transports","title":"Transports","text":""},{"location":"mcp/#stdio-transport","title":"<code>stdio</code> transport","text":"<p>Used for running local MCP servers, the configuration supports the <code>command</code>, <code>args</code>, <code>env</code> &amp; <code>cwd</code> parameters. For example for the git MCP server you would add something like the following to the <code>mcpServers</code> section of the <code>oterm</code> configuration file:</p> <pre><code>{\n  ...\n  \"mcpServers\": {\n    \"git\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"--mount\",\n        \"type=bind,src=/Users/ggozad/dev/open-source/oterm,dst=/oterm\",\n        \"mcp/git\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"mcp/#streamable-http-transport","title":"<code>Streamable HTTP</code> transport","text":"<p>Typically used to connect to remote MCP servers through Streamable HTTP, the only accepted parameter is the <code>url</code> parameter (should start with <code>http://</code> or <code>https://</code>). For example,</p> <pre><code>{\n  ...\n  \"mcpServers\": {\n    \"my_mcp\": {\n            \"url\": \"http://remote:port/path\"\n        }\n  }\n}\n</code></pre>"},{"location":"mcp/#websocket-transport","title":"<code>Websocket</code> transport","text":"<p>Also used to connect to remote MCP servers, but through websockets. The only accepted parameter is the <code>url</code> parameter (should start with <code>ws://</code> or <code>wss://</code>). For example,</p> <pre><code>{\n  ...\n  \"mcpServers\": {\n    \"my_mcp\": {\n            \"url\": \"wss://remote:port/path\"\n        }\n  }\n}\n</code></pre>"},{"location":"mcp/#authentication","title":"Authentication","text":""},{"location":"mcp/#http-bearer-authentication","title":"HTTP bearer authentication","text":"<p><code>oterm</code> supports HTTP bearer authentication by use of tokens. Use <pre><code>{\n  ...\n  \"mcpServers\": {\n    \"my_mcp\": {\n            \"url\": \"http://remote:port/path\",\n      \"auth\": {\n        \"type\": \"bearer\",\n        \"token\": \"XXX\"\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"tools/","title":"Tools","text":"<p><code>oterm</code> supports integration with tools. Tools are special \"functions\" that can provide external information to the LLM model that it does not otherwise have access to.</p> <p>With tools, you can provide the model with access to the web, run shell commands, perform RAG and more.</p> <p>Use existing Model Context Protocol servers</p> <p>or</p> <p>create your own custom tools.</p>"},{"location":"tools/#custom-tools-with-oterm","title":"Custom tools with oterm","text":"<p>You can create your own custom tools and integrate them with <code>oterm</code>.</p>"},{"location":"tools/#create-a-python-package","title":"Create a python package.","text":"<p>You will need to create a python package that exports a <code>Tool</code> definition as well as a callable function that will be called when the tool is invoked.</p> <p>Here is an example of a simple tool that implements an Oracle. The tool is defined in the <code>oracle</code> package which exports the <code>OracleTool</code> tool definition and an <code>oracle</code> callable function.</p> <pre><code>from ollama import Tool\n\nOracleTool = Tool(\n    type=\"function\",\n    function=Tool.Function(\n        name=\"oracle\",\n        description=\"Function to return the Oracle's answer to any question.\",\n        parameters=Tool.Function.Parameters(\n            type=\"object\",\n            properties={\n                \"question\": Tool.Function.Parameters.Property(\n                    type=\"str\", description=\"The question to ask.\"\n                ),\n            },\n            required=[\"question\"],\n        ),\n    ),\n)\n\n\ndef oracle(question: str):\n    return \"oterm\"\n</code></pre> <p>You need to install the package in the same environment where <code>oterm</code> is installed so that <code>oterm</code> can resolve it.</p> <pre><code>cd oracle\nuv pip install . # or pip install .\n</code></pre>"},{"location":"tools/#register-the-tool-with-oterm","title":"Register the tool with oterm","text":"<p>You can register the tool with <code>oterm</code> by adding the tool definittion and callable to the <code>tools</code> section of the <code>oterm</code> configuration file. You can find the location of the configuration file's directory by running <code>oterm --data-dir</code>.</p> <p><pre><code>{\n    ...\n    \"tools\": [{\n        \"tool\": \"oracle.tool:OracleTool\",\n        \"callable\": \"oracle.tool:oracle\"\n    }]\n}\n</code></pre> Note the notation <code>module:object</code> for the tool and callable.</p> <p>That's it! You can now use the tool in <code>oterm</code> with models that support it.</p>"},{"location":"tools/#built-in-example-tools","title":"Built-in example tools","text":"<p>The following example tools are currently built-in to <code>oterm</code>:</p> <ul> <li><code>think</code> - provides the model with a way to think about a question before answering it. This is useful for complex questions that require reasoning. Use it for adding a \"thinking\" step to the model's response.</li> <li><code>date_time</code> - provides the current date and time in ISO format.</li> <li><code>shell</code> - allows you to run shell commands and use the output as input to the model. Obviously this can be dangerous, so use with caution.</li> </ul> <p>These tools are defined in <code>src/oterm/tools</code>. You can make those tools available and enable them for selection when creating or editing a chat, by adding them to the <code>tools</code> section of the <code>oterm</code> configuration file. You can find the location of the configuration file's directory by running <code>oterm --data-dir</code>. So for example to enable the <code>shell</code> tool, you would add the following to the configuration file:</p> <pre><code>{\n    ...\n    \"tools\": [{\n        \"tool\": \"oterm.tools.think:ThinkTool\",\n        \"callable\": \"oterm.tools.think:think\"\n    }]\n}\n</code></pre>"}]}